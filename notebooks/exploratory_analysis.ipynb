# %% [markdown]
# # Amazon Beauty Products - Exploratory Data Analysis
# 
# ## Executive Summary
# 
# **Dataset**: Amazon Beauty Products Ratings  
# **Records**: 2,023,070 ratings from users  
# **Time Period**: Multiple years  
# **Objective**: Understand user behavior, product popularity, and rating patterns
# 
# ---

# %% [markdown]
# ## Table of Contents
# 
# 1. [Setup & Configuration](#setup)
# 2. [Data Loading & Overview](#loading)
# 3. [Data Quality Assessment](#quality)
# 4. [User Behavior Analysis](#users)
# 5. [Product Analysis](#products)
# 6. [Rating Patterns](#ratings)
# 7. [Temporal Analysis](#temporal)
# 8. [Network Analysis](#network)
# 9. [Statistical Insights](#statistics)
# 10. [Business Implications](#business)

# %% [markdown]
# ## 1. Setup & Configuration <a id="setup"></a>

# %%
# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# Visualization settings
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', lambda x: f'{x:,.2f}')

print("Libraries imported successfully!")

# %% [markdown]
# ## 2. Data Loading & Overview <a id="loading"></a>

# %%
# Load the dataset
print("Loading dataset...")
df = pd.read_csv('data/ratings_Beauty.csv')
print(f"Dataset loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns")

# %%
# Display first few rows
print("First 5 rows:")
display(df.head())

# %%
# Display sample rows from different parts of dataset
print("Random sample of 10 rows:")
display(df.sample(10, random_state=42))

# %%
# Basic dataset information
print("\n" + "="*60)
print("DATASET INFORMATION")
print("="*60)

print("\n1. Column Information:")
print(df.info())

print("\n2. Basic Statistics:")
print(df.describe())

print("\n3. Memory Usage:")
print(f"Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB")

# %% [markdown]
# ## 3. Data Quality Assessment <a id="quality"></a>

# %%
# Check for missing values
print("MISSING VALUES ANALYSIS")
print("="*40)

missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100

missing_df = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage (%)': missing_percentage
})
missing_df = missing_df[missing_df['Missing Values'] > 0]

if len(missing_df) == 0:
    print("âœ… No missing values found!")
else:
    print(missing_df)

# %%
# Check for duplicates
print("\nDUPLICATE RECORDS ANALYSIS")
print("="*40)

# Check for exact duplicates
exact_duplicates = df.duplicated().sum()
print(f"Exact duplicates: {exact_duplicates:,} ({exact_duplicates/len(df)*100:.2f}%)")

# Check for user-product duplicates (same user rating same product multiple times)
user_product_duplicates = df.duplicated(subset=['UserId', 'ProductId']).sum()
print(f"User-Product duplicates: {user_product_duplicates:,} ({user_product_duplicates/len(df)*100:.2f}%)")

if user_product_duplicates > 0:
    print("\nSample of user-product duplicates:")
    dup_samples = df[df.duplicated(subset=['UserId', 'ProductId'], keep=False)].head(10)
    display(dup_samples.sort_values(['UserId', 'ProductId']))

# %%
# Data type validation
print("\nDATA TYPE VALIDATION")
print("="*40)

# Check Rating values
print(f"Rating range: [{df['Rating'].min()}, {df['Rating'].max()}]")
print(f"Unique rating values: {sorted(df['Rating'].unique())}")

# Check Timestamp format
df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')
print(f"\nTimestamp range: {df['Timestamp'].min()} to {df['Timestamp'].max()}")

# Check ID formats
print(f"\nUser ID format examples: {df['UserId'].iloc[0]}, {df['UserId'].iloc[1]}")
print(f"Product ID format examples: {df['ProductId'].iloc[0]}, {df['ProductId'].iloc[1]}")

# %%
# Outlier detection
print("\nOUTLIER DETECTION")
print("="*40)

# Rating outliers using IQR method
Q1 = df['Rating'].quantile(0.25)
Q3 = df['Rating'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

rating_outliers = df[(df['Rating'] < lower_bound) | (df['Rating'] > upper_bound)]
print(f"Rating outliers (IQR method): {len(rating_outliers):,} ({len(rating_outliers)/len(df)*100:.2f}%)")

# Temporal outliers (ratings from suspicious time periods)
date_range = df['Timestamp'].max() - df['Timestamp'].min()
print(f"\nDataset covers: {date_range.days} days")

# %% [markdown]
# ## 4. User Behavior Analysis <a id="users"></a>

# %%
print("USER BEHAVIOR ANALYSIS")
print("="*40)

# Basic user statistics
n_users = df['UserId'].nunique()
print(f"Total unique users: {n_users:,}")
print(f"Average ratings per user: {df['UserId'].value_counts().mean():.2f}")
print(f"Median ratings per user: {df['UserId'].value_counts().median():.0f}")

# %%
# User engagement distribution
user_activity = df['UserId'].value_counts()
print(f"\nMost active user gave {user_activity.max():,} ratings")
print(f"Least active user gave {user_activity.min():,} rating(s)")

# %%
# Create user engagement categories
def categorize_users(rating_count):
    if rating_count == 1:
        return 'One-time Rater'
    elif 2 <= rating_count <= 5:
        return 'Occasional Rater'
    elif 6 <= rating_count <= 20:
        return 'Regular Rater'
    elif 21 <= rating_count <= 100:
        return 'Frequent Rater'
    else:
        return 'Power Rater'

user_stats = df.groupby('UserId').agg({
    'Rating': ['count', 'mean', 'std', 'min', 'max'],
    'Timestamp': ['min', 'max']
}).round(2)

user_stats.columns = ['rating_count', 'avg_rating', 'rating_std', 
                      'min_rating', 'max_rating', 'first_rating', 'last_rating']

user_stats['rating_range'] = user_stats['max_rating'] - user_stats['min_rating']
user_stats['rating_consistency'] = 1 - user_stats['rating_std'].fillna(0) / 4
user_stats['user_type'] = user_stats['rating_count'].apply(categorize_users)

# %%
# User engagement visualization
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Plot 1: User rating count distribution (log scale)
axes[0, 0].hist(user_stats['rating_count'], bins=50, log=True, edgecolor='black')
axes[0, 0].set_xlabel('Number of Ratings Given')
axes[0, 0].set_ylabel('Frequency (log scale)')
axes[0, 0].set_title('Distribution of User Activity (Log Scale)')
axes[0, 0].grid(True, alpha=0.3)

# Plot 2: User types pie chart
user_type_counts = user_stats['user_type'].value_counts()
axes[0, 1].pie(user_type_counts.values, labels=user_type_counts.index, 
               autopct='%1.1f%%', startangle=90)
axes[0, 1].set_title('User Types Distribution')

# Plot 3: Average rating by user type
avg_by_type = user_stats.groupby('user_type')['avg_rating'].mean()
axes[0, 2].bar(avg_by_type.index, avg_by_type.values, color='skyblue', edgecolor='navy')
axes[0, 2].set_xlabel('User Type')
axes[0, 2].set_ylabel('Average Rating')
axes[0, 2].set_title('Average Rating by User Type')
axes[0, 2].tick_params(axis='x', rotation=45)
for i, v in enumerate(avg_by_type.values):
    axes[0, 2].text(i, v + 0.05, f'{v:.2f}', ha='center')

# Plot 4: Rating consistency vs rating count
axes[1, 0].scatter(user_stats['rating_count'], user_stats['rating_consistency'], 
                   alpha=0.3, s=10)
axes[1, 0].set_xscale('log')
axes[1, 0].set_xlabel('Number of Ratings (log scale)')
axes[1, 0].set_ylabel('Rating Consistency')
axes[1, 0].set_title('Rating Consistency vs Activity Level')
axes[1, 0].grid(True, alpha=0.3)

# Plot 5: Rating range by user type
range_by_type = user_stats.groupby('user_type')['rating_range'].mean()
axes[1, 1].bar(range_by_type.index, range_by_type.values, color='lightcoral', edgecolor='darkred')
axes[1, 1].set_xlabel('User Type')
axes[1, 1].set_ylabel('Average Rating Range')
axes[1, 1].set_title('Rating Range by User Type')
axes[1, 1].tick_params(axis='x', rotation=45)

# Plot 6: User activity timeline (cumulative users)
df_sorted = df.sort_values('Timestamp')
cumulative_users = df_sorted.groupby(df_sorted['Timestamp'].dt.to_period('M'))['UserId'].nunique().cumsum()
axes[1, 2].plot(cumulative_users.index.astype(str), cumulative_users.values, marker='o')
axes[1, 2].set_xlabel('Month')
axes[1, 2].set_ylabel('Cumulative Unique Users')
axes[1, 2].set_title('Cumulative User Growth Over Time')
axes[1, 2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# %%
# Top users analysis
print("\nTOP 10 MOST ACTIVE USERS:")
print("="*40)
top_users = user_stats.nlargest(10, 'rating_count')[['rating_count', 'avg_rating', 'rating_range', 'user_type']]
display(top_users)

# %%
# User rating patterns analysis
print("\nUSER RATING PATTERNS:")
print("="*40)

# Analyze users who only give extreme ratings
extreme_raters = user_stats[(user_stats['min_rating'] <= 2) & (user_stats['max_rating'] >= 4)]
print(f"Users who give both low (â‰¤2) and high (â‰¥4) ratings: {len(extreme_raters):,}")

# Analyze consistent raters
consistent_raters = user_stats[user_stats['rating_range'] == 0]
print(f"Users who always give same rating: {len(consistent_raters):,}")
if len(consistent_raters) > 0:
    print(f"  Most common consistent rating: {consistent_raters['avg_rating'].mode().values[0]}")

# %% [markdown]
# ## 5. Product Analysis <a id="products"></a>

# %%
print("\nPRODUCT ANALYSIS")
print("="*40)

# Basic product statistics
n_products = df['ProductId'].nunique()
print(f"Total unique products: {n_products:,}")
print(f"Average ratings per product: {df['ProductId'].value_counts().mean():.2f}")
print(f"Median ratings per product: {df['ProductId'].value_counts().median():.0f}")

# %%
# Product popularity distribution
product_activity = df['ProductId'].value_counts()
print(f"\nMost popular product has {product_activity.max():,} ratings")
print(f"Least popular product has {product_activity.min():,} rating(s)")

# %%
# Create product statistics
product_stats = df.groupby('ProductId').agg({
    'Rating': ['count', 'mean', 'std', 'min', 'max'],
    'UserId': 'nunique',
    'Timestamp': ['min', 'max']
}).round(2)

product_stats.columns = ['rating_count', 'avg_rating', 'rating_std', 
                         'min_rating', 'max_rating', 'unique_users',
                         'first_rating', 'last_rating']

product_stats['rating_range'] = product_stats['max_rating'] - product_stats['min_rating']
product_stats['rating_velocity'] = product_stats['rating_count'] / ((product_stats['last_rating'] - product_stats['first_rating']).dt.days + 1)
product_stats['user_to_rating_ratio'] = product_stats['unique_users'] / product_stats['rating_count']

# Categorize products by popularity
def categorize_products(rating_count):
    if rating_count == 1:
        return 'Niche (1 rating)'
    elif 2 <= rating_count <= 10:
        return 'Emerging (2-10 ratings)'
    elif 11 <= rating_count <= 100:
        return 'Popular (11-100 ratings)'
    elif 101 <= rating_count <= 1000:
        return 'Very Popular (101-1000 ratings)'
    else:
        return 'Blockbuster (1000+ ratings)'

product_stats['popularity_category'] = product_stats['rating_count'].apply(categorize_products)

# %%
# Product analysis visualization
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Plot 1: Product rating count distribution
axes[0, 0].hist(product_stats['rating_count'], bins=50, log=True, edgecolor='black')
axes[0, 0].set_xlabel('Number of Ratings Received')
axes[0, 0].set_ylabel('Frequency (log scale)')
axes[0, 0].set_title('Distribution of Product Popularity (Log Scale)')
axes[0, 0].grid(True, alpha=0.3)

# Plot 2: Popularity categories
pop_category_counts = product_stats['popularity_category'].value_counts().sort_index()
axes[0, 1].bar(pop_category_counts.index, pop_category_counts.values, color='lightgreen', edgecolor='darkgreen')
axes[0, 1].set_xlabel('Popularity Category')
axes[0, 1].set_ylabel('Number of Products')
axes[0, 1].set_title('Products by Popularity Category')
axes[0, 1].tick_params(axis='x', rotation=45)
for i, v in enumerate(pop_category_counts.values):
    axes[0, 1].text(i, v + 50, f'{v:,}', ha='center', fontsize=9)

# Plot 3: Average rating vs number of ratings
axes[0, 2].scatter(product_stats['rating_count'], product_stats['avg_rating'], 
                    alpha=0.3, s=10)
axes[0, 2].set_xscale('log')
axes[0, 2].set_xlabel('Number of Ratings (log scale)')
axes[0, 2].set_ylabel('Average Rating')
axes[0, 2].set_title('Average Rating vs Popularity')
axes[0, 2].axhline(y=product_stats['avg_rating'].mean(), color='r', linestyle='--', alpha=0.5)
axes[0, 2].grid(True, alpha=0.3)

# Plot 4: Rating consistency vs popularity
product_stats_filtered = product_stats[product_stats['rating_count'] > 1]
product_stats_filtered['rating_cv'] = product_stats_filtered['rating_std'] / product_stats_filtered['avg_rating']
axes[1, 0].scatter(product_stats_filtered['rating_count'], product_stats_filtered['rating_cv'], 
                    alpha=0.3, s=10)
axes[1, 0].set_xscale('log')
axes[1, 0].set_xlabel('Number of Ratings (log scale)')
axes[1, 0].set_ylabel('Coefficient of Variation')
axes[1, 0].set_title('Rating Consistency vs Popularity')
axes[1, 0].grid(True, alpha=0.3)

# Plot 5: Rating velocity distribution
rating_velocity_filtered = product_stats[product_stats['rating_velocity'] < product_stats['rating_velocity'].quantile(0.95)]
axes[1, 1].hist(rating_velocity_filtered['rating_velocity'], bins=50, edgecolor='black')
axes[1, 1].set_xlabel('Ratings per Day')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].set_title('Distribution of Rating Velocity (95th percentile)')

# Plot 6: Average rating by popularity category
avg_rating_by_category = product_stats.groupby('popularity_category')['avg_rating'].mean()
axes[1, 2].bar(avg_rating_by_category.index, avg_rating_by_category.values, color='orange', edgecolor='darkorange')
axes[1, 2].set_xlabel('Popularity Category')
axes[1, 2].set_ylabel('Average Rating')
axes[1, 2].set_title('Average Rating by Popularity Category')
axes[1, 2].tick_params(axis='x', rotation=45)
for i, v in enumerate(avg_rating_by_category.values):
    axes[1, 2].text(i, v + 0.05, f'{v:.2f}', ha='center')

plt.tight_layout()
plt.show()

# %%
# Top products analysis
print("\nTOP 10 MOST POPULAR PRODUCTS:")
print("="*40)
top_products = product_stats.nlargest(10, 'rating_count')[['rating_count', 'avg_rating', 'unique_users', 'rating_velocity']]
display(top_products)

# %%
print("\nTOP 10 HIGHEST RATED PRODUCTS (min 10 ratings):")
print("="*40)
high_rated = product_stats[product_stats['rating_count'] >= 10].nlargest(10, 'avg_rating')
display(high_rated[['rating_count', 'avg_rating', 'rating_std', 'unique_users']])

# %%
print("\nTOP 10 MOST CONTROVERSIAL PRODUCTS (high std, min 20 ratings):")
print("="*40)
controversial = product_stats[product_stats['rating_count'] >= 20].nlargest(10, 'rating_std')
display(controversial[['rating_count', 'avg_rating', 'rating_std', 'rating_range']])

# %% [markdown]
# ## 6. Rating Patterns Analysis <a id="ratings"></a>

# %%
print("\nRATING PATTERNS ANALYSIS")
print("="*40)

# Overall rating distribution
rating_dist = df['Rating'].value_counts().sort_index()
print("Overall Rating Distribution:")
print(rating_dist)

# %%
# Rating patterns visualization
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Plot 1: Overall rating distribution
axes[0, 0].bar(rating_dist.index.astype(str), rating_dist.values, color='lightblue', edgecolor='navy')
axes[0, 0].set_xlabel('Rating')
axes[0, 0].set_ylabel('Count')
axes[0, 0].set_title('Overall Rating Distribution')
axes[0, 0].grid(True, alpha=0.3)
for i, v in enumerate(rating_dist.values):
    axes[0, 0].text(i, v + 1000, f'{v:,}', ha='center', fontsize=9)

# Plot 2: Rating distribution by user type
rating_by_user_type = pd.crosstab(df['Rating'], df.merge(user_stats['user_type'], 
                                                         left_on='UserId', 
                                                         right_index=True)['user_type'], 
                                 normalize='columns')
rating_by_user_type.T.plot(kind='bar', stacked=True, ax=axes[0, 1])
axes[0, 1].set_xlabel('User Type')
axes[0, 1].set_ylabel('Proportion')
axes[0, 1].set_title('Rating Distribution by User Type')
axes[0, 1].legend(title='Rating', bbox_to_anchor=(1.05, 1))
axes[0, 1].tick_params(axis='x', rotation=45)

# Plot 3: Rating distribution by product popularity
df_with_pop = df.merge(product_stats['popularity_category'], left_on='ProductId', right_index=True)
rating_by_popularity = pd.crosstab(df_with_pop['Rating'], df_with_pop['popularity_category'], 
                                  normalize='columns')
rating_by_popularity.T.plot(kind='bar', stacked=True, ax=axes[0, 2])
axes[0, 2].set_xlabel('Product Popularity Category')
axes[0, 2].set_ylabel('Proportion')
axes[0, 2].set_title('Rating Distribution by Product Popularity')
axes[0, 2].legend(title='Rating', bbox_to_anchor=(1.05, 1))
axes[0, 2].tick_params(axis='x', rotation=45)

# Plot 4: Rating trends over time (monthly average)
df['YearMonth'] = df['Timestamp'].dt.to_period('M')
monthly_avg = df.groupby('YearMonth')['Rating'].mean()
axes[1, 0].plot(monthly_avg.index.astype(str), monthly_avg.values, marker='o', linewidth=2)
axes[1, 0].set_xlabel('Month')
axes[1, 0].set_ylabel('Average Rating')
axes[1, 0].set_title('Monthly Average Rating Trend')
axes[1, 0].grid(True, alpha=0.3)
axes[1, 0].tick_params(axis='x', rotation=45)

# Plot 5: Rating proportion over time
monthly_rating_dist = pd.crosstab(df['YearMonth'], df['Rating'], normalize='index')
axes[1, 1].stackplot(monthly_rating_dist.index.astype(str), 
                     monthly_rating_dist.values.T, 
                     labels=monthly_rating_dist.columns)
axes[1, 1].set_xlabel('Month')
axes[1, 1].set_ylabel('Proportion')
axes[1, 1].set_title('Rating Proportion Over Time')
axes[1, 1].legend(title='Rating', bbox_to_anchor=(1.05, 1))
axes[1, 1].tick_params(axis='x', rotation=45)

# Plot 6: Rating patterns heatmap (hour of day vs day of week)
df['Hour'] = df['Timestamp'].dt.hour
df['DayOfWeek'] = df['Timestamp'].dt.day_name()
rating_by_time = df.groupby(['DayOfWeek', 'Hour'])['Rating'].mean().unstack()
days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
rating_by_time = rating_by_time.reindex(days_order)
im = axes[1, 2].imshow(rating_by_time, aspect='auto', cmap='YlOrRd')
axes[1, 2].set_xlabel('Hour of Day')
axes[1, 2].set_ylabel('Day of Week')
axes[1, 2].set_title('Average Rating by Time (Heatmap)')
axes[1, 2].set_xticks(range(24))
axes[1, 2].set_xticklabels(range(24))
plt.colorbar(im, ax=axes[1, 2])

plt.tight_layout()
plt.show()

# %%
# Advanced rating patterns analysis
print("\nADVANCED RATING PATTERNS:")
print("="*40)

# Rating extremity analysis
df['rating_extremity'] = abs(df['Rating'] - 3)  # Distance from neutral rating 3
print(f"Average rating extremity: {df['rating_extremity'].mean():.2f}")
print(f"Proportion of extreme ratings (1 or 5): {(df['Rating'].isin([1, 5])).mean()*100:.1f}%")

# Rating sequence analysis (for users with multiple ratings)
user_rating_sequences = df.sort_values(['UserId', 'Timestamp']).groupby('UserId')['Rating'].apply(list)
sequence_stats = user_rating_sequences.apply(lambda x: pd.Series({
    'sequence_length': len(x),
    'rating_change_avg': np.mean(np.abs(np.diff(x))) if len(x) > 1 else 0,
    'rating_trend': np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0  # slope
}))

print(f"\nAverage rating change between consecutive ratings: {sequence_stats['rating_change_avg'].mean():.2f}")
print(f"Users with increasing rating trend: {(sequence_stats['rating_trend'] > 0.1).sum():,}")
print(f"Users with decreasing rating trend: {(sequence_stats['rating_trend'] < -0.1).sum():,}")

# %% [markdown]
# ## 7. Temporal Analysis <a id="temporal"></a>

# %%
print("\nTEMPORAL ANALYSIS")
print("="*40)

# Extract temporal features
df['Year'] = df['Timestamp'].dt.year
df['Month'] = df['Timestamp'].dt.month
df['Day'] = df['Timestamp'].dt.day
df['DayOfWeek'] = df['Timestamp'].dt.dayofweek
df['Hour'] = df['Timestamp'].dt.hour
df['WeekOfYear'] = df['Timestamp'].dt.isocalendar().week

# %%
# Temporal patterns visualization
fig, axes = plt.subplots(3, 2, figsize=(15, 15))

# Plot 1: Ratings by year
yearly_counts = df['Year'].value_counts().sort_index()
axes[0, 0].bar(yearly_counts.index.astype(str), yearly_counts.values, color='skyblue', edgecolor='navy')
axes[0, 0].set_xlabel('Year')
axes[0, 0].set_ylabel('Number of Ratings')
axes[0, 0].set_title('Ratings by Year')
axes[0, 0].grid(True, alpha=0.3)
for i, v in enumerate(yearly_counts.values):
    axes[0, 0].text(i, v + 1000, f'{v:,}', ha='center', fontsize=9)

# Plot 2: Ratings by month
monthly_counts = df['Month'].value_counts().sort_index()
month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
axes[0, 1].bar(month_names, monthly_counts.values, color='lightgreen', edgecolor='darkgreen')
axes[0, 1].set_xlabel('Month')
axes[0, 1].set_ylabel('Number of Ratings')
axes[0, 1].set_title('Ratings by Month (All Years)')
axes[0, 1].grid(True, alpha=0.3)

# Plot 3: Ratings by day of week
dow_counts = df['DayOfWeek'].value_counts().sort_index()
dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
axes[1, 0].bar(dow_names, dow_counts.values, color='lightcoral', edgecolor='darkred')
axes[1, 0].set_xlabel('Day of Week')
axes[1, 0].set_ylabel('Number of Ratings')
axes[1, 0].set_title('Ratings by Day of Week')
axes[1, 0].grid(True, alpha=0.3)

# Plot 4: Ratings by hour of day
hourly_counts = df['Hour'].value_counts().sort_index()
axes[1, 1].bar(hourly_counts.index, hourly_counts.values, color='gold', edgecolor='darkgoldenrod')
axes[1, 1].set_xlabel('Hour of Day')
axes[1, 1].set_ylabel('Number of Ratings')
axes[1, 1].set_title('Ratings by Hour of Day')
axes[1, 1].grid(True, alpha=0.3)

# Plot 5: Monthly growth (cumulative ratings)
df_sorted = df.sort_values('Timestamp')
cumulative_ratings = df_sorted.groupby(df_sorted['Timestamp'].dt.to_period('M')).size().cumsum()
axes[2, 0].plot(cumulative_ratings.index.astype(str), cumulative_ratings.values, 
                marker='o', linewidth=2, color='purple')
axes[2, 0].set_xlabel('Month')
axes[2, 0].set_ylabel('Cumulative Ratings')
axes[2, 0].set_title('Cumulative Ratings Over Time')
axes[2, 0].grid(True, alpha=0.3)
axes[2, 0].tick_params(axis='x', rotation=45)

# Plot 6: Average rating by temporal factors
temporal_factors = ['Year', 'Month', 'DayOfWeek', 'Hour']
avg_ratings_by_factor = {}
for factor in temporal_factors:
    avg_ratings_by_factor[factor] = df.groupby(factor)['Rating'].mean()

# Create subplot for average ratings
colors = ['red', 'blue', 'green', 'orange']
for idx, (factor, values) in enumerate(avg_ratings_by_factor.items()):
    axes[2, 1].plot(values.index, values.values, marker='o', label=factor, color=colors[idx])

axes[2, 1].set_xlabel('Factor Value')
axes[2, 1].set_ylabel('Average Rating')
axes[2, 1].set_title('Average Rating by Temporal Factors')
axes[2, 1].legend()
axes[2, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# %%
# Seasonality analysis
print("\nSEASONALITY ANALYSIS:")
print("="*40)

# Monthly pattern analysis (excluding incomplete years)
complete_years = df['Year'].value_counts()[df['Year'].value_counts() > 1000].index
monthly_pattern = df[df['Year'].isin(complete_years)].groupby(['Year', 'Month']).size().unstack()

print("Monthly rating patterns (normalized):")
monthly_normalized = monthly_pattern.div(monthly_pattern.sum(axis=1), axis=0)
display(monthly_normalized.style.background_gradient(cmap='Blues'))

# Weekly patterns
weekly_pattern = df.groupby(['Year', 'WeekOfYear']).size()
print(f"\nAverage ratings per week: {weekly_pattern.mean():.1f}")
print(f"Std of weekly ratings: {weekly_pattern.std():.1f}")

# %% [markdown]
# ## 8. Network Analysis <a id="network"></a>

# %%
print("\nNETWORK ANALYSIS")
print("="*40)

# Create user-product bipartite network statistics
print("Creating network analysis...")

# User-product connections
user_product_counts = df.groupby(['UserId', 'ProductId']).size().reset_index(name='weight')

# Degree analysis
user_degree = df.groupby('UserId')['ProductId'].nunique()
product_degree = df.groupby('ProductId')['UserId'].nunique()

print(f"\nNetwork Statistics:")
print(f"Total nodes (users + products): {len(user_degree) + len(product_degree):,}")
print(f"Total edges (ratings): {len(df):,}")
print(f"Average user degree (products rated): {user_degree.mean():.2f}")
print(f"Average product degree (users rating): {product_degree.mean():.2f}")

# %%
# Network visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Plot 1: User degree distribution
axes[0, 0].hist(user_degree, bins=50, log=True, edgecolor='black', alpha=0.7)
axes[0, 0].set_xlabel('Number of Products Rated (User Degree)')
axes[0, 0].set_ylabel('Frequency (log scale)')
axes[0, 0].set_title('User Degree Distribution')
axes[0, 0].grid(True, alpha=0.3)

# Plot 2: Product degree distribution
axes[0, 1].hist(product_degree, bins=50, log=True, edgecolor='black', alpha=0.7)
axes[0, 1].set_xlabel('Number of Users Rating (Product Degree)')
axes[0, 1].set_ylabel('Frequency (log scale)')
axes[0, 1].set_title('Product Degree Distribution')
axes[0, 1].grid(True, alpha=0.3)

# Plot 3: Degree correlation scatter plot
user_product_pairs = df.groupby(['UserId', 'ProductId']).size().reset_index()
user_stats_merged = user_stats[['rating_count']].rename(columns={'rating_count': 'user_degree'})
product_stats_merged = product_stats[['rating_count']].rename(columns={'rating_count': 'product_degree'})

sample_pairs = user_product_pairs.sample(min(10000, len(user_product_pairs)), random_state=42)
sample_pairs = sample_pairs.merge(user_stats_merged, left_on='UserId', right_index=True)
sample_pairs = sample_pairs.merge(product_stats_merged, left_on='ProductId', right_index=True)

axes[1, 0].scatter(sample_pairs['user_degree'], sample_pairs['product_degree'], 
                   alpha=0.3, s=10)
axes[1, 0].set_xscale('log')
axes[1, 0].set_yscale('log')
axes[1, 0].set_xlabel('User Degree (log scale)')
axes[1, 0].set_ylabel('Product Degree (log scale)')
axes[1, 0].set_title('User-Product Degree Correlation')
axes[1, 0].grid(True, alpha=0.3)

# Plot 4: Network centralization
from collections import Counter

# Calculate degree centrality for users
user_centrality = user_degree / user_degree.max()
axes[1, 1].hist(user_centrality, bins=50, edgecolor='black', alpha=0.7)
axes[1, 1].set_xlabel('Degree Centrality (Normalized)')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].set_title('User Degree Centrality Distribution')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# %%
# Power law analysis
print("\nPOWER LAW ANALYSIS:")
print("="*40)

from scipy import stats

# Fit power law to user degree distribution
user_degree_values = user_degree.values
user_degree_log = np.log(user_degree_values[user_degree_values > 0])
slope, intercept, r_value, p_value, std_err = stats.linregress(
    np.log(np.arange(1, len(user_degree_log) + 1)), 
    np.sort(user_degree_log)[::-1]
)

print(f"User degree power law exponent: {abs(slope):.3f}")
print(f"R-squared: {r_value**2:.3f}")
print(f"p-value: {p_value:.3e}")

# Fit power law to product degree distribution
product_degree_values = product_degree.values
product_degree_log = np.log(product_degree_values[product_degree_values > 0])
slope_prod, intercept_prod, r_value_prod, p_value_prod, std_err_prod = stats.linregress(
    np.log(np.arange(1, len(product_degree_log) + 1)), 
    np.sort(product_degree_log)[::-1]
)

print(f"\nProduct degree power law exponent: {abs(slope_prod):.3f}")
print(f"R-squared: {r_value_prod**2:.3f}")
print(f"p-value: {p_value_prod:.3e}")

# %% [markdown]
# ## 9. Statistical Insights <a id="statistics"></a>

# %%
print("\nSTATISTICAL INSIGHTS")
print("="*40)

# Statistical tests and insights
from scipy import stats

# 1. Test if ratings differ by user type
print("1. ANOVA Test: Do ratings differ by user type?")
user_type_ratings = []
for user_type in user_stats['user_type'].unique():
    user_ids = user_stats[user_stats['user_type'] == user_type].index
    ratings = df[df['UserId'].isin(user_ids)]['Rating']
    user_type_ratings.append(ratings)

f_stat, p_value = stats.f_oneway(*user_type_ratings)
print(f"   F-statistic: {f_stat:.3f}, p-value: {p_value:.3e}")
print(f"   Interpretation: {'Significant difference' if p_value < 0.05 else 'No significant difference'}")

# 2. Test correlation between product popularity and average rating
print("\n2. Correlation: Product popularity vs Average rating")
corr, p_value = stats.pearsonr(product_stats['rating_count'], product_stats['avg_rating'])
print(f"   Pearson correlation: {corr:.3f}, p-value: {p_value:.3e}")
print(f"   Interpretation: {'Significant correlation' if p_value < 0.05 else 'No significant correlation'}")

# 3. Test if rating distributions are normal
print("\n3. Normality Test: Are ratings normally distributed?")
stat, p_value = stats.normaltest(df['Rating'])
print(f"   D'Agostino K^2 test: statistic={stat:.3f}, p-value={p_value:.3e}")
print(f"   Interpretation: {'Not normal' if p_value < 0.05 else 'Normal'}")

# 4. Test seasonality in ratings
print("\n4. Seasonality Test: Do ratings vary by month?")
monthly_ratings = [group['Rating'].values for name, group in df.groupby('Month')]
f_stat, p_value = stats.f_oneway(*monthly_ratings)
print(f"   F-statistic: {f_stat:.3f}, p-value: {p_value:.3e}")
print(f"   Interpretation: {'Seasonal pattern' if p_value < 0.05 else 'No seasonal pattern'}")

# %%
# Statistical summaries
print("\nSTATISTICAL SUMMARIES")
print("="*40)

# Create comprehensive summary table
summary_stats = pd.DataFrame({
    'Metric': [
        'Total Ratings', 'Unique Users', 'Unique Products',
        'Average Rating', 'Rating Std', 'Rating Skewness',
        'User-Product Density', 'Average Ratings per User',
        'Average Ratings per Product', 'Median Ratings per User',
        'Median Ratings per Product', 'Power User Threshold (top 10%)',
        'Long-tail Products (1 rating)'
    ],
    'Value': [
        f"{len(df):,}",
        f"{df['UserId'].nunique():,}",
        f"{df['ProductId'].nunique():,}",
        f"{df['Rating'].mean():.3f}",
        f"{df['Rating'].std():.3f}",
        f"{df['Rating'].skew():.3f}",
        f"{len(df) / (df['UserId'].nunique() * df['ProductId'].nunique()) * 100:.6f}%",
        f"{df.groupby('UserId').size().mean():.2f}",
        f"{df.groupby('ProductId').size().mean():.2f}",
        f"{df.groupby('UserId').size().median():.0f}",
        f"{df.groupby('ProductId').size().median():.0f}",
        f"{df.groupby('UserId').size().quantile(0.9):.0f}+ ratings",
        f"{product_stats[product_stats['rating_count'] == 1].shape[0]:,}"
    ]
})

display(summary_stats)

# %%
# Key ratios and metrics
print("\nKEY RATIOS AND METRICS:")
print("="*40)

ratios = {
    'Users to Products Ratio': df['UserId'].nunique() / df['ProductId'].nunique(),
    'Ratings per User': len(df) / df['UserId'].nunique(),
    'Ratings per Product': len(df) / df['ProductId'].nunique(),
    'Sparsity (1 - Density)': 1 - (len(df) / (df['UserId'].nunique() * df['ProductId'].nunique())),
    'Gini Coefficient (User Activity)': gini_coefficient(user_stats['rating_count']),
    'Gini Coefficient (Product Popularity)': gini_coefficient(product_stats['rating_count']),
    'Percentage of 5-star Ratings': (df['Rating'] == 5).mean() * 100,
    'Percentage of 1-star Ratings': (df['Rating'] == 1).mean() * 100,
    'Average Days Between Ratings': (df.groupby('UserId')['Timestamp'].apply(lambda x: x.diff().dt.days.mean())).mean()
}

for metric, value in ratios.items():
    if isinstance(value, float):
        if 'Percentage' in metric or 'Sparsity' in metric:
            print(f"{metric}: {value:.2%}")
        elif 'Gini' in metric:
            print(f"{metric}: {value:.3f}")
        elif 'Ratio' in metric or 'per' in metric:
            print(f"{metric}: {value:.2f}")
        else:
            print(f"{metric}: {value:.2f}")
    else:
        print(f"{metric}: {value}")

# Helper function for Gini coefficient
def gini_coefficient(x):
    """Calculate Gini coefficient of inequality"""
    x = np.sort(x)
    n = len(x)
    cumx = np.cumsum(x, dtype=float)
    return (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n

# %% [markdown]
# ## 10. Business Implications & Recommendations <a id="business"></a>

# %%
print("\nBUSINESS IMPLICATIONS & RECOMMENDATIONS")
print("="*60)

# Create a comprehensive insights report
insights = [
    {
        'category': 'User Behavior',
        'insights': [
            f"**Power Law Distribution**: User activity follows a power law - {user_stats['rating_count'].quantile(0.9):.0f}+ ratings define top 10% 'power users'",
            f"**User Segmentation**: {user_stats['user_type'].value_counts()['One-time Rater']:,} one-time raters ({user_stats['user_type'].value_counts()['One-time Rater']/len(user_stats)*100:.1f}%) represent opportunity for re-engagement",
            f"**Rating Consistency**: {sequence_stats[sequence_stats['rating_change_avg'] < 0.5].shape[0]:,} users are consistent raters (avg change < 0.5 stars)",
            f"**Extreme Raters**: {(df['Rating'].isin([1, 5])).mean()*100:.1f}% of ratings are extremes (1 or 5 stars), indicating polarized opinions"
        ],
        'recommendations': [
            "Implement user segmentation strategy for personalized engagement",
            "Create loyalty programs for power users to increase retention",
            "Develop re-engagement campaigns for one-time raters",
            "Monitor extreme raters for potential review manipulation"
        ]
    },
    {
        'category': 'Product Performance',
        'insights': [
            f"**Long Tail Effect**: {product_stats[product_stats['rating_count'] == 1].shape[0]:,} products ({product_stats[product_stats['rating_count'] == 1].shape[0]/len(product_stats)*100:.1f}%) have only 1 rating",
            f"**Blockbuster Products**: Top {len(top_products)} products account for {top_products['rating_count'].sum()/len(df)*100:.1f}% of all ratings",
            f"**Rating Velocity**: Average of {product_stats['rating_velocity'].mean():.2f} ratings per day per product (median: {product_stats['rating_velocity'].median():.2f})",
            f"**Controversial Products**: {len(controversial):,} products have high rating variance (Ïƒ > {controversial['rating_std'].min():.2f})"
        ],
        'recommendations': [
            "Focus marketing efforts on blockbuster products for maximum ROI",
            "Implement cross-selling strategies for long-tail products",
            "Monitor rating velocity to identify trending products early",
            "Investigate controversial products for quality issues"
        ]
    },
    {
        'category': 'Rating Patterns',
        'insights': [
            f"**Positive Bias**: Average rating is {df['Rating'].mean():.2f} (above neutral 3.0), with {rating_dist[5]/len(df)*100:.1f}% 5-star ratings",
            f"**Temporal Patterns**: {df.groupby('Hour')['Rating'].mean().idxmax()} AM has highest average ratings ({df.groupby('Hour')['Rating'].mean().max():.2f})",
            f"**Weekly Patterns**: {dow_names[df.groupby('DayOfWeek')['Rating'].mean().idxmax()]} has highest average rating ({df.groupby('DayOfWeek')['Rating'].mean().max():.2f})",
            f"**Seasonality**: {'Significant' if p_value < 0.05 else 'No significant'} monthly variation in ratings (ANOVA p={p_value:.3e})"
        ],
        'recommendations': [
            "Schedule product launches during high-rating time periods",
            "Adjust customer service based on weekly rating patterns",
            "Monitor seasonal rating trends for inventory planning",
            "Implement time-based recommendation algorithms"
        ]
    },
    {
        'category': 'Network Analysis',
        'insights': [
            f"**High Sparsity**: User-product matrix is {ratios['Sparsity (1 - Density)']:.6%} sparse, typical for recommendation systems",
            f"**Inequality**: Gini coefficient of {ratios['Gini Coefficient (User Activity)']:.3f} indicates high inequality in user activity",
            f"**Power Law**: User and product degree distributions follow power law (exponent: {abs(slope):.3f})",
            f"**Network Structure**: Average user connects to {user_degree.mean():.2f} products, average product connects to {product_degree.mean():.2f} users"
        ],
        'recommendations': [
            "Use collaborative filtering to handle sparsity effectively",
            "Implement hybrid recommendation systems for cold-start problems",
            "Leverage network structure for community detection",
            "Use power law properties for efficient sampling"
        ]
    }
]

# Display insights
for category in insights:
    print(f"\nðŸ“Š {category['category']}")
    print("-" * 40)
    print("\nKey Insights:")
    for insight in category['insights']:
        print(f"  â€¢ {insight}")
    print("\nRecommendations:")
    for rec in category['recommendations']:
        print(f"  âœ“ {rec}")

# %%
# Create final summary visualization
print("\n" + "="*60)
print("EXECUTIVE SUMMARY VISUALIZATION")
print("="*60)

fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# KPI 1: User engagement pyramid
user_type_counts = user_stats['user_type'].value_counts()
axes[0, 0].barh(list(reversed(user_type_counts.index)), list(reversed(user_type_counts.values)), 
                color=['darkred', 'red', 'orange', 'yellow', 'lightgreen'])
axes[0, 0].set_xlabel('Number of Users')
axes[0, 0].set_title('User Engagement Pyramid')
for i, v in enumerate(list(reversed(user_type_counts.values))):
    axes[0, 0].text(v + 50, i, f'{v:,}', va='center')

# KPI 2: Product popularity distribution
pop_labels = ['Blockbuster', 'Very Popular', 'Popular', 'Emerging', 'Niche']
pop_values = product_stats['popularity_category'].value_counts().reindex(['Blockbuster (1000+ ratings)', 
                                                                          'Very Popular (101-1000 ratings)',
                                                                          'Popular (11-100 ratings)', 
                                                                          'Emerging (2-10 ratings)', 
                                                                          'Niche (1 rating)']).values
axes[0, 1].pie(pop_values, labels=pop_labels, autopct='%1.1f%%', startangle=90)
axes[0, 1].set_title('Product Popularity Distribution')

# KPI 3: Rating distribution
rating_proportions = df['Rating'].value_counts(normalize=True).sort_index() * 100
colors = ['darkred', 'red', 'orange', 'lightgreen', 'darkgreen']
axes[1, 0].bar(rating_proportions.index.astype(str), rating_proportions.values, color=colors)
axes[1, 0].set_xlabel('Rating')
axes[1, 0].set_ylabel('Percentage (%)')
axes[1, 0].set_title('Rating Distribution')
for i, v in enumerate(rating_proportions.values):
    axes[1, 0].text(i, v + 1, f'{v:.1f}%', ha='center')

# KPI 4: Temporal growth
monthly_growth = df.groupby(df['Timestamp'].dt.to_period('M')).size()
axes[1, 1].plot(monthly_growth.index.astype(str), monthly_growth.values, marker='o', linewidth=2)
axes[1, 1].set_xlabel('Month')
axes[1, 1].set_ylabel('Number of Ratings')
axes[1, 1].set_title('Monthly Rating Activity')
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# %%
# Save analysis results
print("\nSaving analysis results...")

# Save key statistics to CSV
user_stats.to_csv('output/user_statistics.csv')
product_stats.to_csv('output/product_statistics.csv')
summary_stats.to_csv('output/summary_statistics.csv')

# Save insights to markdown
with open('output/key_insights.md', 'w') as f:
    f.write("# Amazon Beauty Products - Key Insights\n\n")
    for category in insights:
        f.write(f"## {category['category']}\n\n")
        f.write("### Insights:\n")
        for insight in category['insights']:
            f.write(f"- {insight}\n")
        f.write("\n### Recommendations:\n")
        for rec in category['recommendations']:
            f.write(f"- {rec}\n")
        f.write("\n")

print("Analysis complete! Results saved to 'output/' directory")
print("\n" + "="*60)
print("ðŸŽ¯ NEXT STEPS:")
print("="*60)
print("1. Feature Engineering: Create derived features for modeling")
print("2. Model Development: Implement recommendation algorithms")
print("3. Evaluation: Test models with appropriate metrics")
print("4. Deployment: Create production-ready recommendation system")
